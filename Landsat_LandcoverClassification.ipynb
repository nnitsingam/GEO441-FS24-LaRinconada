{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Cover Classification for Landsat Collection 2 Level 2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook belongs to the Spring Semester 2024, GEO441 Remote Sensing Seminar Project \"Time Series Analysis of Mining Activities and Related\n",
    "Pollution in La Rinconada, Peru Using Satellite Imagery\" and provides the python code to the methodology for landcover classification, which is described in the seminar projects report. \n",
    "\n",
    "**Remarks:** \n",
    "- All data can be found in the Microsoft Teams Group 'Team Peru - Mining'. Please download the folder called 'data.zip', unzip it and store it in the same directory as this notebook.\n",
    "- If the notebook is not accessed trough GitHub directly, please also download the python file 'fast_glcm' and store it in the same directory as this notebook as well. The file stems from a GitHub repository (Taka, 2022) providing a fast version of GLCM calculations.\n",
    "- This notebook uses some python modules that might be difficult to install consecutively, due to inconsistency between package dependencies. It is therefor recommended to use a conda environment and the 'environment_larinconada.yml' file (also in this GitHub repository) to install the necessary infrastructure to run the classification.\n",
    "\n",
    "**Source**:\n",
    "*Taka, I. (2022).* Fast gray-level co-occurrence matrix by numpy. Retrieved 21.05.2024, from https://github.com/tzm030329/GLCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import geometry_mask\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fiona\n",
    "from sklearn import (model_selection, metrics)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from shapely.geometry import Point\n",
    "from scipy.ndimage import generic_filter\n",
    "from scipy.stats import mode\n",
    "import fast_glcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize bands into 0.0 - 1.0 scale\n",
    "def normalize(array):\n",
    "    array_min, array_max = array.min(), array.max()\n",
    "    return (array - array_min) / (array_max - array_min)\n",
    "\n",
    "# Create a RGB image\n",
    "def get_rgb(raster):\n",
    "    red = raster.read(3)\n",
    "    green = raster.read(2)\n",
    "    blue = raster.read(1)\n",
    "    \n",
    "    # Normalize band DN\n",
    "    red_norm = normalize(red)\n",
    "    green_norm = normalize(green)\n",
    "    blue_norm = normalize(blue)\n",
    "    \n",
    "    # Stack bands\n",
    "    nrg = np.dstack((red_norm, green_norm, blue_norm))\n",
    "\n",
    "    #View the color composite\n",
    "    plt.imshow(nrg)\n",
    "\n",
    "# Create a False Color Composite\n",
    "def get_fcc(raster):\n",
    "    nir = raster.read(4)\n",
    "    red = raster.read(3)\n",
    "    green = raster.read(2)\n",
    "    \n",
    "    # Normalize band DN\n",
    "    nir_norm = normalize(nir)\n",
    "    red_norm = normalize(red)\n",
    "    green_norm = normalize(green)\n",
    "    \n",
    "    # Stack bands\n",
    "    nrg = np.dstack((nir_norm, red_norm, green_norm))\n",
    "    \n",
    "    # View the color composite\n",
    "    plt.imshow(nrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please choose what sample dataset to use. \n",
    "- To classify images from 1999-2012, use the \"Landsat7_groundcovers.shp\".\n",
    "- To classify images from 2013-2023, use the \"Landsat8_groundcovers.shp\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pixel center shapefile\n",
    "point_grid_gdf = gpd.read_file(r\"data/input_data/Landsat_30m_PointGrid.shp\")\n",
    "\n",
    "#Insert dataset path for either Landsat 7 or Landsat 8!\n",
    "ground_samples_gdf = gpd.read_file(r\"data/input_data/Landsat7_groundcovers.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section the landcover samples are preprocessed and prepared for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2'\n",
    "              ,'blue_glcm', 'green_glcm', 'red_glcm', 'nir_glcm', 'swir1_glcm', 'swir2_glcm'\n",
    "              ,'ndvi','ndwi', 'smi']\n",
    "band_indices = [1, 2, 3, 4, 5, 6, 7 , 8, 9, 10, 11, 12, 13, 14, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Point Layer that contains only points within the our polygon samples and join the band values to the points. This creates a point layer that will later be split into test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all years represented in the ground samples, initialize a dataframe containing all necessary columns for an \n",
    "# initial version of the training and test dataframe.\n",
    "ground_sample_years = list(np.unique(ground_samples_gdf.year))\n",
    "sample_points = pd.DataFrame({\n",
    "    'id': [],\n",
    "    'geometry': [],\n",
    "    'index_right': [],\n",
    "    'land_cover': [],\n",
    "    'year': [],\n",
    "    'blue': [],\n",
    "    'green': [],\n",
    "    'red': [],\n",
    "    'nir': [],\n",
    "    'swir1': [],\n",
    "    'swir2': [],\n",
    "    'blue_glcm':[],\n",
    "    'green_glcm':[],\n",
    "    'red_glcm':[],\n",
    "    'nir_glcm':[],\n",
    "    'swir1_glcm':[],\n",
    "    'swir2_glcm':[],\n",
    "    'ndwi':[],\n",
    "    'ndvi':[],\n",
    "    'smi':[]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, for each year...\n",
    "**1)** the pixel centers that are contained within a sample polygon from that respective year are extracted,\n",
    "**2)** the landsat image of this year is opened and the bands are normalized, indices and glcm entropy is calculated and all those bands are stored in a new raster image,\n",
    "**3)** the new raster image is reopened, and intersected with the pixel centers extracted in step 1, then added to the collection of all sample points.\n",
    "\n",
    "**4)** After those steps, the full sample_points dataframe is balanced by the smallest number of landcover classes contained in it.\n",
    "\n",
    "In the subsequent cells the inputs will further be normalized and the coordinates added to the input dataset. Finally the test and training classes are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in ground_sample_years:\n",
    "    #step 1\n",
    "    sample_point_year = gpd.sjoin(point_grid_gdf, ground_samples_gdf[ground_samples_gdf.year == year], how='inner', predicate='intersects')\n",
    "    coord_list = [(x, y) for x, y in zip(sample_point_year[\"geometry\"].x, sample_point_year[\"geometry\"].y)]\n",
    "\n",
    "    #step 2\n",
    "    with rasterio.open(r\"data/landsat_images/PE_LX-sr_30m_%s_composite_filt.tif\"%str(year)) as src:\n",
    "        blue = normalize(src.read(1).astype(float))\n",
    "        green = normalize(src.read(2).astype(float))\n",
    "        red = normalize(src.read(3).astype(float))\n",
    "        nir = normalize(src.read(4).astype(float))\n",
    "        swir1 = normalize(src.read(5).astype(float))\n",
    "        swir2 = normalize(src.read(6).astype(float))\n",
    "\n",
    "        ndwi = (green - nir) / (green + nir)\n",
    "        ndvi = (nir - red) / (nir + red)\n",
    "        smi = (nir - swir1) / (nir + swir1)\n",
    " \n",
    "        # Get metadata of the original image\n",
    "        profile = src.profile\n",
    "        \n",
    "        # Update metadata to include index and glcm entropy bands\n",
    "        profile.update(\n",
    "            count=src.count + 9,  # Increase band count by 9\n",
    "            dtype=rasterio.float32  # Set data type of new band to float32\n",
    "        )\n",
    "        \n",
    "        # Calculate glcm entropy for each original landsat band        \n",
    "        glcm_means = []\n",
    "        for i in range(6):\n",
    "            img = src.read(i+1)\n",
    "            h, w = img.shape\n",
    "            mean_glcm = fast_glcm.fast_glcm_entropy(img) # Calculate pixel entropy\n",
    "            glcm_means.append(mean_glcm)\n",
    "\n",
    "        # After reading, close the raster file before opening it for writing\n",
    "        image_path = r\"data/landsat_images/PE_LX-sr_30m_%s_composite_filt_extended.tif\" % str(year)\n",
    "        \n",
    "        with rasterio.open(image_path, 'w', **profile) as dst:\n",
    "            # Write the original bands to the updated image\n",
    "            for i in range(1, src.count + 1):\n",
    "                dst.write(normalize(src.read(i)), i)\n",
    "            # Write glcm bands to updated image    \n",
    "            for i in range(6):\n",
    "                dst.write(glcm_means[i], src.count+(i+1))\n",
    "            # Write indices to updated image\n",
    "            dst.write(ndvi, src.count + 7)\n",
    "            dst.write(ndwi, src.count + 8)\n",
    "            dst.write(smi, src.count + 9)\n",
    "            \n",
    "   #step 3\n",
    "    with rasterio.open(image_path) as image:\n",
    "        for i in band_indices: \n",
    "            sample_point_year[band_names[i-1]] = [x[0] for x in image.sample(coord_list, indexes=[i])]\n",
    "        sample_points = pd.concat([sample_point_year, sample_points], ignore_index=True)\n",
    "\n",
    "#step 4\n",
    "# Get the class labels and count the number of samples per class\n",
    "class_counts = sample_points['land_cover'].value_counts()\n",
    "\n",
    "# Find the class with the lowest number of samples\n",
    "min_class = class_counts.idxmin()\n",
    "min_samples = class_counts[min_class]\n",
    "\n",
    "# Create a balanced dataset\n",
    "balanced_dataset = pd.DataFrame()\n",
    "\n",
    "for class_label in class_counts.index:\n",
    "    # Get the samples for the current class\n",
    "    class_samples = sample_points[sample_points['land_cover'] == class_label]\n",
    "    \n",
    "    # If the class has more samples than the minimum, randomly select the minimum number of samples\n",
    "    if len(class_samples) > min_samples:\n",
    "        balanced_class_samples = class_samples.sample(n=min_samples, random_state=42)\n",
    "    else:\n",
    "        balanced_class_samples = class_samples\n",
    "    \n",
    "    # Append the balanced class samples to the balanced dataset\n",
    "    balanced_dataset = pd.concat([balanced_dataset, balanced_class_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "sample_points = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'index_right' in sample_points.columns:\n",
    "        sample_points.drop(columns=['index_right'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize GLCM Bands between 0 and 1\n",
    "glcm_bands=['blue_glcm', 'green_glcm', 'red_glcm', 'nir_glcm', 'swir1_glcm', 'swir2_glcm']\n",
    "for band in glcm_bands:\n",
    "    sample_points[band] = sample_points[band]/np.max(sample_points[band])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse mean band values per land cover\n",
    "means_per_land_cover = sample_points.groupby('land_cover')[band_names].mean()\n",
    "\n",
    "# Plotting line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for land_cover, data in means_per_land_cover.iterrows():\n",
    "    plt.plot(data.index, data.values, label=land_cover)\n",
    "\n",
    "plt.xlabel('Band')\n",
    "plt.ylabel('Mean Values')\n",
    "plt.title('Mean Values of Bands per Land Cover')\n",
    "plt.legend(title='Land Cover')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pixel coordinates to the training and test datasample\n",
    "training_bands = band_names.copy()\n",
    "training_bands.append('X')\n",
    "training_bands.append('Y')\n",
    "# Normalize Coordinates between 0 and 1\n",
    "sample_points['X'] = sample_points.geometry.x/np.max(sample_points.geometry.x)\n",
    "sample_points['Y'] = sample_points.geometry.y/np.max(sample_points.geometry.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and Test Data Sets\n",
    "X = sample_points[training_bands].fillna(0)\n",
    "y = sample_points['land_cover']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation of Classifier\n",
    "lc_classifier= RandomForestClassifier(random_state=100)\n",
    "\n",
    "lc_classifier.fit(X_train, y_train)\n",
    "score_training = lc_classifier.score(X_train,y_train)\n",
    "score_test = lc_classifier.score(X_test,y_test)\n",
    "\n",
    "# Print Overall Accuracy\n",
    "print(\"Overall Accuracy: %.2f %%\"%(score_test*100))\n",
    "print(\"Training Accuracy: %.2f %%\"%(score_training*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Quality Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classifier = lc_classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "print(cm_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points['land_cover'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize cm_norm\n",
    "import seaborn as sns\n",
    "\n",
    "colors = ['#FF0000', '#00FF00']  \n",
    "cmap = sns.color_palette(colors)\n",
    "labels = sample_points['land_cover'].unique()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "figure = plt.gcf()\n",
    "ax = figure.add_subplot(111)\n",
    "sns.heatmap(cm_norm, cmap=cmap, annot=True, annot_kws={\"size\": 8}, xticklabels=labels, yticklabels=labels)  # fmt='g' stellt sicher, dass die Werte als Zahlen angezeigt werden\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Ground Truth')\n",
    "ax.set_title(f'Confusion Matrix for Random Forest Classifier')\n",
    "plt.savefig('cm_rf_landsat7.png',  bbox_inches='tight', pad_inches=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision and Recall\n",
    "from sklearn.metrics import classification_report\n",
    "y_true, y_pred, = y_test, classifier.predict(X_test)\n",
    "report = classification_report(y_true, y_pred, digits=2, target_names=labels, output_dict=True)\n",
    "\n",
    "report = pd.DataFrame(report).transpose()\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Landcover Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First choose the years to predict. Then, the images are prepared like the training data, except that the intersection \n",
    "with the training samples is omitted. With those pixel centers and their input values in a dataframe, the classification can be\n",
    "carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose years to predict\n",
    "# Landsat 7 = list(range(1999, 2013))\n",
    "# Landsat 8 = list(range(2013, 2024))\n",
    "years = list(range(1999, 2013))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing input classification datastructure\n",
    "landcover_identification = {\n",
    "    'land_cover': ['barren land', 'bare rock', 'vegetation', 'mining area (land)', 'mining area (water)', 'urban area', 'water',\n",
    "       'snow', 'shadow'],\n",
    "    'lc_id': [1, 2, 3, 4, 5, 6, 7, 8,9]\n",
    "}\n",
    "landcover_identification = pd.DataFrame(landcover_identification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was initially used to smooth the classification with a 3x3 majority filter. \n",
    "# The smoothened version was never used for any analysis, but still is in the output images as an eight band.\n",
    "def majority_filter(neighborhood):\n",
    "    return mode(neighborhood, axis=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    if (year not in ground_sample_years):\n",
    "        with rasterio.open(r\"data/landsat_images/PE_LX-sr_30m_%s_composite_filt.tif\"%str(year)) as src:\n",
    "            blue = normalize(src.read(1).astype(float))\n",
    "            green = normalize(src.read(2).astype(float))\n",
    "            red = normalize(src.read(3).astype(float))\n",
    "            nir = normalize(src.read(4).astype(float))\n",
    "            swir1 = normalize(src.read(5).astype(float))\n",
    "            swir2 = normalize(src.read(6).astype(float))\n",
    "            ndwi = (green - nir) / (green + nir)\n",
    "            ndvi = (nir - red) / (nir + red)\n",
    "            smi = (nir - swir1) / (nir + swir1) \n",
    "    \n",
    "            # Get metadata of the original image\n",
    "            profile = src.profile\n",
    "            profile.update(\n",
    "                count=src.count + 9,  # Increase band count by 9\n",
    "                dtype=rasterio.float32  # Set data type of new band to float32\n",
    "            )\n",
    "            \n",
    "            glcm_means = []\n",
    "            for i in range(6):\n",
    "                img = src.read(i+1)\n",
    "                h, w = img.shape\n",
    "                mean_glcm = fast_glcm.fast_glcm_entropy(img)\n",
    "                glcm_means.append(mean_glcm)\n",
    "\n",
    "    \n",
    "        # After reading, close the raster file before opening it for writing\n",
    "            image_path = r\"data/landsat_images/PE_LX-sr_30m_%s_composite_filt_extended.tif\" % str(year)\n",
    "            \n",
    "            with rasterio.open(image_path, 'w', **profile) as dst:\n",
    "                # Write the original bands to the updated image\n",
    "                for i in range(1, src.count + 1):\n",
    "                    dst.write(normalize(src.read(i)), i)\n",
    "\n",
    "                for i in range(6):\n",
    "                    dst.write(glcm_means[i], src.count+(i+1))\n",
    "                # Write indices\n",
    "                dst.write(ndvi, src.count + 7)\n",
    "                dst.write(ndwi, src.count + 8)\n",
    "                dst.write(smi, src.count + 9)\n",
    "                    \n",
    "    # Sample pixel centers over newly saved image to retrieve band values per pixel in a dataframe-like structure\n",
    "    with rasterio.open(r\"data/landsat_images/PE_LX-sr_30m_%s_composite_filt_extended.tif\"%str(year)) as dataset_to_classify:\n",
    "        input_points = point_grid_gdf\n",
    "        \n",
    "        coord_list = [(x, y) for x, y in zip(input_points[\"geometry\"].x, input_points[\"geometry\"].y)]\n",
    "        \n",
    "        for i in band_indices: \n",
    "            input_points[band_names[i-1]] = [x[0] for x in dataset_to_classify.sample(coord_list, indexes=[i])]\n",
    "\n",
    "    if 'index_right' in input_points.columns:\n",
    "            input_points.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    # Remove all empty rows\n",
    "    non_zero_rows = input_points.loc[~(input_points[band_names] == 0).all(axis=1)]\n",
    "    input_points_c = non_zero_rows\n",
    "\n",
    "    input_points_c['X'] = input_points_c.geometry.x/np.max(input_points_c.geometry.x)\n",
    "    input_points_c['Y'] = input_points_c.geometry.y/np.max(input_points_c.geometry.y)\n",
    "    \n",
    "    # Predict values, then merge coordinates, bands and classification back together\n",
    "    X_to_predict = input_points_c[training_bands]\n",
    "    X_to_predict = X_to_predict.fillna(0)\n",
    "    y_pred = lc_classifier.predict(X_to_predict)\n",
    "    output_points = input_points_c\n",
    "    output_points['land_cover'] = y_pred\n",
    "   \n",
    "    # Join landcover identification with the output points\n",
    "    mask = (output_points[['blue', 'green', 'red', 'nir', 'swir1', 'swir2']] == 0).all(axis=1)\n",
    "    output_points.loc[mask, 'land_cover'] = \"shadow\"\n",
    "    output_points = output_points.merge(landcover_identification, on='land_cover')\n",
    "    \n",
    "    # Get Metadata of initial raster image\n",
    "    with rasterio.open(r\"data/landsat_images/PE_LX-sr_30m_%s_composite_filt.tif\"%str(year)) as src:\n",
    "        # Read the raster dataset as a NumPy array\n",
    "        raster_array = src.read()\n",
    "        # Get the metadata of the raster dataset\n",
    "        meta = src.meta\n",
    "    \n",
    "    # Extract coordinates and attribute values from points\n",
    "    point_coords = [(x, y) for x, y in zip(output_points.geometry.x, output_points.geometry.y)]\n",
    "    attribute_values = output_points['lc_id'].tolist()\n",
    "    \n",
    "    # Convert point coordinates to pixel coordinates\n",
    "    pixel_coords = [src.index(x, y) for x, y in point_coords]\n",
    "    \n",
    "    # Create new layer in raster dataset \n",
    "    lc_id = np.empty((1,raster_array.shape[1], raster_array.shape[2]))\n",
    "    raster_array = np.append(raster_array, lc_id, axis = 0)\n",
    "    \n",
    "    # Assign attribute values to corresponding raster pixels\n",
    "    for (col, row), value in zip(pixel_coords, attribute_values):\n",
    "        raster_array[6, col, row] = value\n",
    "    \n",
    "    # Create a new layer in the raster dataset\n",
    "    new_layer_name = 'lc_id'\n",
    "    meta['count'] += 1  # Increment the number of bands\n",
    "    meta['dtype'] = raster_array.dtype  # Update the data type\n",
    "    # Add smoothened version of landcover classification\n",
    "    meta['count'] += 1 \n",
    "    lc_smooth = np.empty((1,raster_array.shape[1], raster_array.shape[2]))\n",
    "    raster_array = np.append(raster_array, lc_smooth, axis = 0) \n",
    "    raster_array[7] = generic_filter(raster_array[6], majority_filter, size=3)\n",
    "    \n",
    "    \n",
    "    # Write the modified raster dataset with the new layer added\n",
    "    with rasterio.open(r\"data/outputs/landsat_%s_LC_RF.tif\"%str(year), 'w', **meta) as dst:\n",
    "        dst.write(raster_array)\n",
    "        \n",
    "    print(\"New layer added to raster dataset from %s.\"%str(year))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
